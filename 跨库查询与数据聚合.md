# 跨库查询与数据聚合

## 核心要点（面试快速回答）

**Q: 分库后如何跨库查询？**
用Elasticsearch。MySQL负责存储和一致性，ES负责搜索和聚合。通过Canal监听binlog实时同步，延迟50-100ms，用户无感知。

**Q: 为什么选ES不选其他方案？**
因为需要全文检索、聚合统计、相关性排序，这些ES都擅长。如果扫描256个分库，至少2-3秒，ES查询P99只要100ms。

**Q: 数据一致性怎么保证？**
三个机制：1）版本号解决乱序；2）每天凌晨全量校验；3）同步失败记录补偿表定时重试。实际一致性达到99.99%。

**Q: 其他跨库场景怎么处理？**
聚合统计用预聚合表+Redis；全局ID查询用路由表或ID中包含路由信息；复杂分析走离线数仓。

---

## 一、问题背景与核心挑战

### 1.1 业务场景

分了256个库之后，最大的挑战是**跨商家的商品查询**。典型场景：
- 用户搜索"汉堡"，需要查询所有商家的商品
- 涉及256个分库，每个库平均2000万商品
- 需要聚合、排序、分页
- 要求响应时间 < 200ms

### 1.2 为什么不能直接查DB？

**如果扫描所有分库：**
- 256次数据库查询，串行执行至少2-3秒
- 即使并行查询，也需要500ms+
- 数据库压力巨大（256个库同时查询）
- 内存占用高（需要加载所有结果到应用层）
- 排序和分页困难（需要在应用层做全局排序）

**结论：分库分表后，跨库查询必须引入其他技术方案。**

---

## 二、解决方案：MySQL + Elasticsearch

### 2.1 为什么选择ES？

**技术选型考量：**

1. **全文检索能力**
   - 支持中文分词（ik分词器）
   - 支持模糊搜索、同义词
   - 相关性排序

2. **聚合能力强**
   - 按分类、价格区间聚合
   - 统计平均价格、销量
   - 性能远超MySQL

3. **水平扩展**
   - 分片机制，轻松扩展到PB级
   - 副本机制，保证高可用

4. **查询性能**
   - 倒排索引，查询速度快
   - P99延迟 < 100ms

### 2.2 整体架构

```
MySQL（256个分库）
  ↓ Canal监听binlog
  ↓ 实时同步
Elasticsearch集群（50台）
  ↓ 提供搜索服务
应用层
```

**职责划分：**
- **MySQL**：存储完整数据，保证数据一致性，作为数据源
- **Elasticsearch**：提供搜索、聚合、排序能力，作为查询引擎
- **Canal**：实时同步数据变更，保证数据最终一致

---

## 三、数据同步方案

### 3.1 同步链路设计

**Canal + MQ 的异步同步方案：**

```
MySQL binlog → Canal → RocketMQ → ES Consumer → Elasticsearch
```

**为什么加MQ？**
1. **解耦**：Canal和ES之间解耦，ES故障不影响Canal
2. **削峰**：高峰期写入，MQ缓冲
3. **重试**：消息失败可以重试
4. **顺序保证**：同一商品的消息发到同一队列

### 3.2 同步流程

**步骤：**
1. Canal监听256个分库的binlog
2. 解析INSERT/UPDATE/DELETE事件
3. 构建同步消息，发送到MQ
4. ES Consumer消费消息，写入ES
5. 失败记录到补偿表，定时重试

**同步延迟：**
- 正常情况：50-100ms
- 高峰期：100-200ms
- 可接受范围：用户无感知

---

## 四、数据一致性保证

### 4.1 核心问题

**MySQL和ES之间的一致性挑战：**
1. **同步延迟**：Canal监听binlog有延迟
2. **消息丢失**：MQ可能丢消息（极端情况）
3. **同步失败**：ES写入可能失败（网络、容量）
4. **顺序问题**：并发更新可能乱序

### 4.2 解决方案

**方案1：版本号机制（解决乱序问题）**

在MySQL表中增加version字段，每次更新version+1。ES同步时检查版本号，只有新版本才更新。这样即使消息乱序到达，也能保证最终一致。

**方案2：定时全量校验（兜底方案）**

每天凌晨3点，从MySQL分页查询所有商品，与ES对比。发现不一致的数据，以MySQL为准修复ES。这是最后的保障机制。

**方案3：实时补偿机制**

同步失败时，记录到补偿表。定时任务每分钟扫描补偿表，重新从MySQL查询数据，同步到ES。最多重试3次，超过3次人工介入。

**实际效果：**
- 数据一致性：99.99%
- 不一致窗口：< 5分钟
- 每天补偿数据：< 1000条

---

## 五、ES索引设计

### 5.1 索引结构

**核心字段：**
- product_id：商品ID
- merchant_id：商家ID
- product_name：商品名称（text类型，ik分词）
- category_id：分类ID
- price：价格（用于排序）
- sales：销量（用于排序）
- rating：评分
- status：状态（用于过滤）

**设计要点：**
1. product_name使用text+keyword双类型，支持全文检索和精确匹配
2. price、sales、rating用于排序和聚合
3. status用于过滤（只查在售商品）

### 5.2 分片策略

**分片数量：**
- 主分片：20个
- 副本：2个
- 总分片数：60个

**为什么是20个主分片？**
- 50亿商品，每个分片2.5亿
- 单分片大小约50GB，性能最优
- 预留扩展空间

---

## 六、性能优化

### 6.1 缓存策略

**热门搜索词缓存：**
- 识别Top100热门搜索词
- 缓存搜索结果，TTL=5分钟
- 命中率约40%

**效果：**
- 热门词查询延迟从80ms降到5ms
- ES集群压力降低40%

### 6.2 ES查询优化

**优化技巧：**
1. **使用filter代替query**：filter有缓存，性能更好
2. **控制返回字段**：只返回必要字段，减少网络传输
3. **使用scroll API**：处理大结果集，避免深分页

---

## 七、其他跨库查询场景

### 7.1 按商家维度的聚合统计

**场景：统计每个商家的商品数量、总销量**

这种场景不适合用ES，原因：
- 商家数量有限（百万级）
- 需要精确统计（不能有误差）
- 实时性要求高

**解决方案：预聚合表**

维护一个单独的商家统计表，商品变更时实时更新。使用Redis计数，定期刷新到DB。

### 7.2 全局ID查询

**场景：根据商品ID查询，但不知道在哪个分库**

**方案1：路由表**
维护一个全局路由表，记录商品ID到分库的映射。这个表很小，可以全部缓存在Redis。

**方案2：ID中包含路由信息**
商品ID生成时，高位包含分库信息。查询时直接从ID解析出分库索引。

---

## 八、方案对比与选型

### 8.1 不同场景的技术选型

| 查询场景 | 推荐方案 | 原因 |
|---------|---------|------|
| 关键词搜索 | Elasticsearch | 全文检索能力强 |
| 聚合统计 | 预聚合表 + Redis | 实时性高，精确 |
| 全局ID查询 | 路由表 + 缓存 | 简单高效 |
| 复杂分析 | 数据仓库（离线） | 不影响在线服务 |

### 8.2 实际效果

**性能指标：**
- 搜索响应时间：P99 < 100ms
- ES同步延迟：平均 80ms
- 数据一致性：99.99%
- 搜索可用性：99.95%

**成本：**
- ES集群：50台（相比256个DB，成本可控）
- 存储成本：增加约20%（ES索引）
- 运维成本：中等（需要ES专业知识）

### 8.3 踩过的坑

**1. ES写入慢导致消息堆积**
- 问题：高峰期ES写入TPS不够，MQ消息堆积
- 解决：增加ES节点，优化bulk写入批次大小

**2. 深分页性能问题**
- 问题：用户翻到第100页，查询超时
- 解决：限制最大翻页数，引导用户使用筛选条件

**3. 数据不一致被用户发现**
- 问题：商品刚上架，搜索不到
- 解决：关键操作（上架）同步写ES，不走异步
