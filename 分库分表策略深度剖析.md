# 分库分表策略深度剖析

## 核心要点（面试快速回答）

**Q: 为什么按商家ID分片？**
因为75%的查询是"查某个商家的所有商品"，按商家ID分片可以避免跨库查询，事务也更简单。虽然按商品ID查询需要先查路由表，但这只占20%，可以接受。

**Q: 遇到热点商家怎么办？**
三个方案：1）大商家二次分片到4个子表；2）热点识别+缓存预热；3）读写分离，热点库加从库。实施后热点库QPS从15000降到5000。

**Q: 数据迁移怎么做？**
双写+灰度切流。新数据同时写新老库，历史数据异步迁移，通过灰度逐步切流量（1%→10%→50%→100%），全程数据校验，耗时3个月，业务无感知。

**Q: 跨SET数据同步？**
基于Canal+MQ，正常延迟50-100ms。数据不一致时，通过版本号机制保证最终一致，关键操作会强制从主SET读取。

---

## 一、为什么选择按商家ID分片而不是商品ID？

### 1.1 业务场景分析

在外卖场景下，我们分析了核心查询场景的占比：

**查询场景统计（基于线上日志分析）：**
```
1. 查询某个商家的所有商品（商家商品列表）：占比 75%
   - 用户浏览餐厅菜单
   - 商家管理自己的商品
   
2. 查询单个商品详情（按商品ID）：占比 20%
   - 用户点击商品查看详情
   - 订单系统查询商品信息
   
3. 跨商家的商品搜索：占比 5%
   - 用户搜索"汉堡"、"奶茶"等
   - 这类查询走搜索引擎（ES），不走DB
```

### 1.2 决策依据

**1. 避免跨库查询**
- 如果按商品ID分片，查询"某个商家的所有商品"需要扫描所有分库
- 按商家ID分片，同一商家的所有商品在同一个库，单库查询即可

**2. 事务边界清晰**
- 商家批量上架/下架商品，都在同一个库内，可以用本地事务
- 如果按商品ID分片，批量操作需要分布式事务，复杂度高

**3. 缓存友好**
- 商家维度的缓存更容易命中（用户倾向于在同一个商家浏览多个商品）
- 商品维度的缓存命中率相对较低

### 1.3 技术权衡

**优势：**
- 主场景（75%）性能最优，单库查询
- 事务简单，无需分布式事务
- 数据局部性好，缓存命中率高

**劣势：**
- 按商品ID查询需要带上商家ID（通过商品ID反查商家ID）
- 可能存在数据倾斜（大商家商品多）

### 1.4 解决方案

**商品表设计：**
```sql
CREATE TABLE product_{0000-0255} (
    product_id BIGINT PRIMARY KEY,
    merchant_id BIGINT NOT NULL,  -- 分片键
    product_name VARCHAR(200),
    price DECIMAL(10,2),
    status TINYINT,
    -- 其他字段
    INDEX idx_merchant_id (merchant_id),
    INDEX idx_product_id (product_id)
);

-- 商品ID到商家ID的映射表（单独的小表）
CREATE TABLE product_merchant_mapping (
    product_id BIGINT PRIMARY KEY,
    merchant_id BIGINT NOT NULL,
    INDEX idx_product_id (product_id)
) ENGINE=InnoDB;
```

**按商品ID查询的实现：**
```java
public Product getProductById(Long productId) {
    // 1. 先从映射表查商家ID（这个表很小，全部缓存在Redis）
    Long merchantId = mappingCache.getMerchantId(productId);
    if (merchantId == null) {
        merchantId = mappingDao.getMerchantId(productId);
    }
    
    // 2. 根据商家ID定位分库，查询商品
    int shardIndex = (int)(merchantId % 256);
    return productDao.selectById(shardIndex, productId);
}
```

---

## 二、热点商家问题与解决方案

### 2.1 问题场景

**是的，遇到过，而且是个大坑。**

**案例1：头部连锁商家**
- 某知名连锁品牌，全国有5000+门店
- 每个门店作为一个商家，但商品模板共享
- 导致某个分库存储了5000+商家的数据，单库数据量是其他库的10倍

**案例2：超级大商家**
- 某大型超市，单个商家有10万+SKU
- 高峰期QPS达到5万+（大促时用户疯狂刷新）
- 单库压力巨大，慢查询频发

### 2.2 数据倾斜的具体表现

我们做过统计：
```
分库数据量分布（256个库）：
- P50：500万商品
- P90：800万商品  
- P99：2000万商品  ← 严重倾斜
- Max：5000万商品  ← 极端情况

单库QPS分布：
- 平均：2000 QPS
- P99：8000 QPS   ← 热点库
- Max：15000 QPS  ← 极端热点
```

### 2.3 解决方案

#### 方案1：二次分片（针对超大商家）

对于商品数超过100万的大商家，在商家维度再做一次分片：

```java
// 分片策略
public int getShardIndex(Long merchantId, Long productId) {
    // 第一层：商家维度分片
    int merchantShard = (int)(merchantId % 256);
    
    // 第二层：判断是否是大商家
    if (isBigMerchant(merchantId)) {
        // 大商家的商品再按商品ID分片到4个子表
        int subTableIndex = (int)(productId % 4);
        return merchantShard * 10 + subTableIndex;  // 逻辑分片
    }
    
    return merchantShard;
}

// 表结构
// 普通商家：product_0000 ~ product_0255
// 大商家：product_0000_0 ~ product_0000_3（4个子表）
```

#### 方案2：热点商家识别与缓存预热

```java
@Component
public class HotMerchantDetector {
    private final LoadingCache<Long, AtomicLong> qpsCounter;
    private final Set<Long> hotMerchants = new ConcurrentHashSet<>();
    
    // 实时统计商家QPS
    public void recordAccess(Long merchantId) {
        AtomicLong counter = qpsCounter.get(merchantId);
        counter.incrementAndGet();
    }
    
    // 每10秒识别一次热点商家
    @Scheduled(fixedRate = 10000)
    public void detectHotMerchants() {
        List<Long> newHotMerchants = qpsCounter.asMap().entrySet().stream()
            .filter(e -> e.getValue().get() > 5000)  // QPS > 5000
            .map(Map.Entry::getKey)
            .collect(Collectors.toList());
        
        // 更新热点商家列表
        hotMerchants.clear();
        hotMerchants.addAll(newHotMerchants);
        
        // 触发缓存预热
        for (Long merchantId : newHotMerchants) {
            preheatCache(merchantId);
        }
        
        // 重置计数器
        qpsCounter.invalidateAll();
    }
    
    private void preheatCache(Long merchantId) {
        // 1. 预热到本地缓存（所有机器）
        List<Product> products = productService.getProductsByMerchant(merchantId);
        localCache.putAll(merchantId, products);
        
        // 2. 预热到Redis（延长过期时间）
        redisCache.setWithExpire(merchantId, products, 3600);  // 1小时
    }
}
```

#### 方案3：读写分离 + 从库负载均衡

针对热点库，增加更多的从库：

```java
// 普通库：1主2从
// 热点库：1主8从  ← 动态扩容

@Component
public class DynamicDataSource {
    private final Map<Integer, List<DataSource>> slaveDataSources;
    
    public DataSource getReadDataSource(Long merchantId) {
        int shardIndex = (int)(merchantId % 256);
        
        // 热点库有更多从库
        List<DataSource> slaves = slaveDataSources.get(shardIndex);
        
        // 负载均衡选择从库
        int slaveIndex = ThreadLocalRandom.current().nextInt(slaves.size());
        return slaves.get(slaveIndex);
    }
}
```

#### 方案4：本地缓存强制命中

热点商家的数据，强制走本地缓存：

```java
public List<Product> getProductsByMerchant(Long merchantId) {
    // 热点商家：强制本地缓存
    if (hotMerchantDetector.isHot(merchantId)) {
        List<Product> products = localCache.get(merchantId);
        if (products != null) {
            return products;
        }
    }
    
    // 普通流程：L1 -> L2 -> DB
    return normalQuery(merchantId);
}
```

#### 方案5：限流保护

对热点商家的查询进行限流：

```java
@Component
public class MerchantRateLimiter {
    private final Map<Long, RateLimiter> limiters = new ConcurrentHashMap<>();
    
    public boolean tryAcquire(Long merchantId) {
        // 普通商家：10000 QPS
        // 热点商家：20000 QPS（更高的阈值，但仍然限流）
        int qpsLimit = hotMerchantDetector.isHot(merchantId) ? 20000 : 10000;
        
        RateLimiter limiter = limiters.computeIfAbsent(
            merchantId, 
            k -> RateLimiter.create(qpsLimit)
        );
        
        return limiter.tryAcquire();
    }
}
```

### 2.4 优化效果

实施以上方案后：
- 热点库的QPS从15000降到5000（降低67%）
- P99响应时间从200ms降到60ms
- 数据库CPU使用率从90%降到50%
- 缓存命中率从60%提升到85%

---

## 三、从16库扩到256库的数据迁移方案

### 3.1 迁移整体方案

这是一个非常复杂的工程，我们花了3个月时间完成。

**核心原则：**
1. 业务无感知（不停服）
2. 数据零丢失
3. 可随时回滚
4. 灰度迁移，风险可控

**整体方案：双写 + 灰度切流**

```
阶段1：准备阶段（1周）
  ├─ 搭建256个新库
  ├─ 数据校验工具开发
  └─ 回滚预案准备

阶段2：双写阶段（4周）
  ├─ 写入同时写老库和新库
  ├─ 读取仍然从老库读
  └─ 异步数据同步（历史数据）

阶段3：数据校验（2周）
  ├─ 全量数据对比
  ├─ 修复不一致数据
  └─ 持续校验

阶段4：灰度切流（4周）
  ├─ 1% 流量切到新库
  ├─ 10% 流量
  ├─ 50% 流量
  └─ 100% 流量

阶段5：清理阶段（1周）
  ├─ 停止双写
  ├─ 下线老库
  └─ 监控观察
```

---

### 3.2 双写实现

```java
@Component
public class DualWriteProductDao {
    private final ProductDao oldDao;  // 16库
    private final ProductDao newDao;  // 256库
    
    public void insert(Product product) {
        Long merchantId = product.getMerchantId();
        int oldShard = (int)(merchantId % 16);
        int newShard = (int)(merchantId % 256);
        
        // 1. 先写老库（主路径）
        oldDao.insert(oldShard, product);
        
        // 2. 异步写新库
        CompletableFuture.runAsync(() -> {
            try {
                newDao.insert(newShard, product);
            } catch (Exception e) {
                logDualWriteFailure(product, e);
            }
        });
    }
    
    public Product selectById(Long merchantId, Long productId) {
        // 根据灰度配置决定从哪个库读
        if (shouldReadFromNewDb(merchantId)) {
            int newShard = (int)(merchantId % 256);
            return newDao.selectById(newShard, productId);
        } else {
            int oldShard = (int)(merchantId % 16);
            return oldDao.selectById(oldShard, productId);
        }
    }
    
    private boolean shouldReadFromNewDb(Long merchantId) {
        // 灰度策略：按商家ID取模
        int grayPercent = config.getGrayPercent();  // 0-100
        return (merchantId % 100) < grayPercent;
    }
}
```


### 3.3 数据一致性保证

**双写期间的一致性保证机制：**

```java
@Component
public class DataConsistencyChecker {
    
    // 实时校验（采样10%）
    public void realtimeCheck(Long merchantId, Long productId) {
        if (ThreadLocalRandom.current().nextInt(100) < 10) {
            CompletableFuture.runAsync(() -> {
                checkDataConsistency(merchantId, productId);
            });
        }
    }
    
    private void checkDataConsistency(Long merchantId, Long productId) {
        int oldShard = (int)(merchantId % 16);
        int newShard = (int)(merchantId % 256);
        
        Product oldProduct = oldDao.selectById(oldShard, productId);
        Product newProduct = newDao.selectById(newShard, productId);
        
        if (!Objects.equals(oldProduct, newProduct)) {
            // 数据不一致，以老库为准修复新库
            logInconsistency(merchantId, productId);
            newDao.update(newShard, oldProduct);
        }
    }
}
```


---

## 四、SET化部署与跨SET数据同步

### 4.1 SET化部署架构

**SET划分策略：**
```
北京SET：服务北京及周边地区
上海SET：服务上海及华东地区
广州SET：服务广州及华南地区
成都SET：服务成都及西南地区
```

**每个SET内包含：**
- 应用服务集群
- 数据库集群（256个分库）
- Redis集群
- 消息队列


### 4.2 跨SET数据同步

**同步方案：基于Canal + MQ**

```java
@Component
public class CrossSetDataSync {
    
    // Canal监听本SET的binlog变更
    @CanalListener(destination = "product_db")
    public void onDataChange(CanalEntry.Entry entry) {
        // 解析binlog
        RowChange rowChange = parseEntry(entry);
        
        // 发送到MQ，同步到其他SET
        for (String targetSet : getOtherSets()) {
            mqProducer.send(
                "data_sync_" + targetSet,
                buildSyncMessage(rowChange)
            );
        }
    }
}
```

**同步延迟：**
- 正常情况：50-100ms
- 网络抖动：200-500ms
- 极端情况：1-2秒


### 4.3 数据不一致处理

**场景1：同步延迟导致的不一致**

用户在北京SET下单，但查询时路由到了上海SET，可能看到旧数据。

**解决方案：**
```java
// 写入时记录版本号
public void updateProduct(Product product) {
    product.setVersion(System.currentTimeMillis());
    productDao.update(product);
}

// 读取时检查版本
public Product getProduct(Long productId) {
    Product product = localSetDao.query(productId);
    
    // 如果本地数据太旧（超过5秒），从主SET读取
    if (System.currentTimeMillis() - product.getVersion() > 5000) {
        return masterSetDao.query(productId);
    }
    
    return product;
}
```

