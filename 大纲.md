# 系统介绍

## 整体定位：  
商品系统是美团外卖交易链路的核心基础服务，承载了百亿级商品数据，为C端用户提供商品信息查询、搜索、详情展示等能力，同时为商家端提供商品管理能力。系统支撑日单量1.5亿，峰值QPS超过100万，集群规模4000+台。

## 核心架构：  
我们采用的是分层架构设计：

1. 接入层：Nginx做流量接入和初步限流，支持地域就近路由
2. 应用层：基于Java的微服务架构，按业务域拆分为商品基础服务、商品详情服务、商品搜索服务等
3. 缓存层：三级缓存架构- L1：本地缓存（Caffeine），命中率约30%，RT在1ms以内  
    - L2：分布式缓存（Redis集群），命中率约60%，RT在5ms左右  
    - L3：DB（MySQL分库分表），兜底查询
4. 存储层：MySQL分库分表，按商家ID哈希分片，单库控制在500万商品以内

数据规模：

- 商品总量：百亿级（包含历史商品）
- 在线商品：约50亿
- 分库数量：1024个库
- 分表策略：按商家ID哈希 + 按时间归档

核心挑战：  
主要是三个方面：一是百亿数据规模下的查询性能，二是缓存与DB的一致性保障，三是C端大流量高并发下的系统稳定性。

## 完整链路
美团外卖从用户打开App到外卖送达的完整链路。

一、整体业务链路（5个核心环节）

```
1. 用户浏览与下单
- 用户打开美团App，通过首页推荐或搜索找到餐厅
- 浏览商品信息（菜品、价格、评价等）
- 加入购物车，确认配送地址和联系方式
- 选择支付方式完成支付

2. 商家接单与备餐

- 订单实时推送到商家端（商家App或POS系统）
- 商家确认接单，开始准备餐品
- 商家标记餐品制作完成

3. 配送员匹配与取餐

- 平台智能调度系统根据骑手位置、订单优先级、配送距离等因素，匹配最合适的配送员
- 配送员接单后前往商家取餐
- 确认餐品完整性后开始配送

4. 配送过程

- 骑手按照智能规划的路线配送
- 实时更新配送状态，用户可在App查看骑手位置
- 平均配送时间约28分钟，配送距离多为3-5公里

5. 送达与评价

- 骑手将餐品送达用户手中
- 用户确认收货
- 用户可对商品质量和配送服务进行评价
```

---

## 技术架构层面的系统协同

从技术视角看，这个链路涉及多个核心系统的协同：

前台系统：

- 用户端App：商品展示、下单、支付、订单跟踪
- 商家端系统：订单接收、餐品管理、营业状态管理
- 骑手端App：订单接收、导航、状态更新

中台核心系统：

- 商品系统：我负责的这块，提供商品信息查询、库存校验、价格计算（含满减、红包等）
- 订单系统：整个链路的"中枢"，管理订单全生命周期，协调各系统
- 营销系统：优惠券、满减、红包等营销活动
- 支付系统：对接多种支付渠道
- 配送调度系统：智能派单、ETA预测、路径规划、骑手匹配

后台支撑系统：

- 库存系统：我也参与的部分，实时库存管理和扣减
- 用户系统：用户信息、地址管理
- 商家系统：商家信息、营业状态
- 风控系统：反作弊、异常订单识别



# 深度讨论

---

## 第一部分：架构设计与演进

### 1. 架构演进路径

问：你提到商品系统支撑百亿级数据和百万QPS，这个系统是一开始就是这个架构，还是逐步演进的？能详细说说架构演进的关键节点吗？每次演进的触发因素是什么  
？技术决策的权衡点在哪里？  


商品系统不是一开始就是现在这个架构，而是随着业务发展经历了四个关键阶段的演进。

---

#### 架构演进的核心经验总结

1. 演进的节奏：

- 不要过度设计，够用就好
- 每次演进都是被业务倒逼的，不是为了技术而技术

2. 技术决策的权衡原则：

- 优先解决当前最痛的问题
- 考虑未来1-2年的业务增长，但不要过度预估
- 成本、性能、复杂度三者平衡

3. 最重要的经验：

- 分库分表的分片键选择要符合业务主场景
- 缓存架构要考虑容灾，不能过度依赖单一缓存
- 限流要精细化，不能一刀切
- 数据冷热分离是降低成本的有效手段

#### a. 第一阶段：单体应用期（日订单量 < 10万）

架构特点：

- 单体应用，商品、订单、商家等功能都在一个工程里
- 单库单表，MySQL主从架构
- 简单的Redis缓存
- 服务器规模：几十台

触发演进的因素：

- 日订单量突破10万，数据库单表数据量超过1000万
- 数据库慢查询频繁，高峰期RT从几十ms飙升到几百ms
- 代码耦合严重，一个模块的问题会影响整个系统

技术决策：  
决定进行垂直拆分，将商品、订单、商家等拆成独立服务。

权衡点：

- 优势：解耦、独立部署、故障隔离
- 代价：引入分布式复杂度、服务间调用延迟
- 为什么这么做：业务快速增长，必须解决单点瓶颈，代价可以接受

---

#### b. 第二阶段：服务化初期（日订单量 10万-100万）

架构特点：

- 商品系统独立出来，成为独立服务
- 开始做分库分表，按商家ID哈希分16个库
- 引入Redis集群做缓存
- 服务器规模：200-300台

触发演进的因素：

- 商品数据量突破10亿，单库性能瓶颈明显
- 缓存命中率低，大量请求打到DB
- 部分大商家成为热点，单库压力过大

技术决策：

1. 分库分表策略：选择按商家ID哈希而不是商品ID  
    - 原因：外卖场景下，90%的查询都是"查询某个商家的所有商品"，按商家ID分片可以避免跨库查询  
    - 权衡：牺牲了按商品ID查询的便利性，但符合业务主场景
2. 缓存架构升级：从单层Redis升级到二级缓存（本地缓存+Redis）

权衡点：

- 分库数量选择16个而不是更多：考虑到运维复杂度和扩容成本
- 接受最终一致性：缓存更新采用异步方式，换取性能

#### c. 第三阶段：高并发优化期（日订单量 100万-1000万）

架构特点：

- 分库扩展到256个库
- 三级缓存架构：本地缓存（Caffeine）+ Redis集群 + DB
- 引入全链路限流体系
- SET化部署，支持异地多活
- 服务器规模：1000-2000台

触发演进的因素：

1. 容量问题：16个库已经不够用，单库数据量超过5000万，性能下降
2. 可用性问题：发生过几次Redis集群故障导致大量请求打到DB，差点把DB打挂
3. 延迟问题：跨地域调用延迟高，用户体验差
4. 流量冲击：午晚高峰流量是平时的5-10倍，经常出现限流降级

关键技术决策：

决策1：分库从16扩到256

- 触发点：单库数据量超过5000万，查询性能明显下降
- 方案：采用双写+灰度切流的方式平滑迁移
- 权衡点：

- 为什么是256而不是128或512？经过容量规划，256个库可以支撑未来3年业务增长，同时256是2的幂次，方便哈希计算
- 代价：数据迁移周期长（3个月），需要保证数据一致性

决策2：引入本地缓存

- 触发点：Redis集群故障导致的雪崩问题
- 方案：在应用层增加本地缓存（Caffeine），TTL设置为30秒
- 权衡点：

- 优势：即使Redis挂了，本地缓存也能抗住大部分流量
- 代价：引入了缓存一致性问题，需要通过Canal+MQ主动刷新
- 为什么TTL是30秒？经过压测，30秒的不一致窗口对业务影响可接受，且能保证较高的命中率

决策3：SET化部署

- 触发点：跨地域调用延迟高（P99达到100ms+）
- 方案：按地域划分SET，每个SET内闭环
- 权衡点：

- 优势：降低延迟70%，提升容灾能力
- 代价：数据需要跨SET同步，增加了复杂度
- 如何选择SET粒度？按城市级别划分，北京、上海、广州等一线城市独立SET，其他城市按区域合并

#### d. 第四阶段：精细化治理期（日订单量 1000万-1.5亿）

架构特点：

- 商品数据达到百亿级
- 峰值QPS突破100万
- 冷热分离，历史数据归档
- 智能限流，区分核心商家和普通商家
- 服务器规模：4000+台

触发演进的因素：

1. 成本问题：数据量暴增，存储和计算成本快速上升
2. 性能问题：虽然做了分库分表和缓存，但P99还是在120ms左右，不够理想
3. 稳定性问题：大促期间流量激增，需要更精细的流量控制

关键技术决策：

决策1：冷热分离

- 触发点：百亿数据中，80%是历史商品（已下架或删除），但占用了大量存储
- 方案：

- 热数据：近3个月活跃的商品，约50亿，存在线上库
- 温数据：3-12个月的商品，存在归档库（SSD）
- 冷数据：12个月以上的商品，存在对象存储（OSS）

- 权衡点：

- 优势：在线库数据量减半，查询性能提升40%，存储成本降低60%
- 代价：查询历史数据需要跨库，RT会变高（但这类查询占比不到1%）

决策2：智能限流

- 触发点：一刀切的限流策略导致大促时核心商家也被限流，影响GMV
- 方案：

- 建立商家分级体系（S级、A级、B级、C级）
- S级商家（头部商家）限流阈值是普通商家的10倍
- 动态调整限流阈值，根据实时流量和系统负载自动调节

- 权衡点：

- 优势：保障核心商家体验，GMV损失减少80%
- 代价：限流逻辑更复杂，需要实时计算和决策

限流是怎么做的

[限流算法汇总](限流算法汇总.md)

[限流是怎么做的](限流是怎么做的.md)

决策3：性能深度优化

- 触发点：P99在120ms，用户体验不够好
- 具体优化手段：  
    a. 慢SQL优化：通过索引优化和SQL改写，慢查询减少90%  
    b. 序列化优化：从JSON改为Protobuf，序列化耗时降低60%  
    c. 连接池优化：调整Redis和DB连接池参数，减少连接等待  
    d. 批量查询优化：将多次查询合并为一次，减少网络开销
- 效果：P99从120ms降到50ms

### 2. 分库分表策略深度剖析  

你提到按商家ID哈希分片，为什么选择商家ID而不是商品ID？这个决策背后的业务场景和技术考量是什么？遇到过热点商家的问题吗？比如某个大商家的商品访量特别高，如何解决单库热点问题？

[分库分表策略深度剖析](分库分表策略深度剖析.md)

### 3. 跨库查询与数据聚合  

分了1024个库之后，如何处理跨商家的商品查询场景？比如用户搜索"汉堡"，需要查询所有商家的商品，你们是怎么设计的？是用搜索引擎（ES）还是其他方案？  
数据一致性如何保证？

[跨库查询与数据聚合](跨库查询与数据聚合.md)

### 4. 百亿数据的冷热分离  

你提到百亿商品包含历史商品，在线商品约50亿。冷热数据是如何划分的？归档策略是什么？归档后的历史数据如何查询？对在线系统性能有什么影响？

[百亿数据的冷热分离](百亿数据的冷热分离.md)

### 5. 容量规划与扩容  

4000+台服务器的集群规模，容量规划是怎么做的？如何评估什么时候需要扩容？扩容时如何保证业务无感知？分库分表的扩容（比如从256库扩到512库）做过吗  
？数据迁移方案是什么？

[容量规划与扩容](容量规划与扩容.md)

---

## 第二部分：缓存架构与一致性（5个问题）

### 1. 三级缓存的设计细节

你提到L1本地缓存命中率30%，L2  
Redis命中率60%。这个命中率是怎么统计的？为什么是这个比例？本地缓存的更新策略是什么？如何避免不同机器的本地缓存不一致？

[三级缓存的设计细节](三级缓存的设计细节.md)

### 2. 缓存穿透、击穿、雪崩  

百万QPS的场景下，缓存穿透、击穿、雪崩这三个经典问题你们是怎么解决的？具体用了哪些技术手段？有没有遇到过线上事故？怎么处理的？

[缓存穿透击穿雪崩](缓存穿透击穿雪崩.md)

### 3. 缓存一致性方案  
  
你提到用Canal+MQ异步刷新缓存，为什么选择最终一致性而不是强一致性？能接受多长时间的不一致窗口？如果Canal延迟或者消息丢失了怎么办？有没有兜底方  
案？

[缓存一致性方案](缓存一致性方案.md)

## 第三部分：高并发与性能优化（5个问题）

### 1. 百万QPS的流量分布
百万QPS是峰值还是平均值？流量在时间维度和空间维度上的分布特征是什么？午高峰和晚高峰的流量比例？不同地域的流量差异？这些特征如何影响你的架构设
计？

[百万QPS的流量分布](百万QPS的流量分布.md)

### 2. 限流策略的分层设计
你提到从接入层到数据层的全链路限流，具体每一层的限流算法是什么？令牌桶、漏桶还是滑动窗口？限流阈值如何确定？如何做到精细化限流（比如区分核心商家和普通商家）？

[限流是怎么做的](限流是怎么做的.md)
[限流算法汇总](限流算法汇总.md)

### 3. 降级与熔断机制
在极端流量或者依赖系统故障的情况下，商品系统的降级策略是什么？哪些功能可以降级？哪些必须保证？降级决策是人工还是自动？熔断后如何恢复？

[降级与熔断机制](降级与熔断机制.md)

### 4. 性能优化的具体手段
你提到P99从120ms优化到50ms，具体做了哪些优化？是代码层面、架构层面还是基础设施层面？能举几个最有效果的优化案例吗？如何量化每个优化的收益？

[性能优化的具体手段](性能优化的具体手段.md)

### 5. 热点数据处理
外卖场景下肯定有热点商品（比如某个爆款奶茶），如何识别热点？热点数据的缓存策略和普通数据有什么不同？有没有做本地缓存的动态预热？

[热点数据处理](热点数据处理.md)

---

## 第四部分：数据一致性与可靠性（5个问题）

### 1. 分布式事务处理
商品系统和库存系统、订单系统之间的数据一致性如何保证？用的是2PC、TCC还是最终一致性方案？能举个具体的业务场景说明吗？

[分布式事务处理](分布式事务处理.md)

### 2. 数据一致性的边界场景
用户下单时，商品信息、价格、库存需要保持一致。如果在下单过程中商品被下架或者价格变更了，如何处理？有没有遇到过用户投诉说下单时看到的价格和实际支付价格不一致？

[数据一致性的边界场景](数据一致性的边界场景.md)

### 3. 数据校验与修复
百亿数据规模下，如何保证缓存、DB、搜索引擎之间的数据一致性？有没有数据校验机制？发现不一致后如何修复？修复的优先级和策略是什么？

[数据校验与修复](数据校验与修复.md)

### 4. 灰度发布与回滚
商品系统这么大的规模，如何做灰度发布？灰度策略是什么（按流量、按地域、按商家）？如果灰度过程中发现问题，如何快速回滚？数据如何处理？

[灰度发布与回滚](灰度发布与回滚.md)

### 5. 监控与故障定位
你提到建立了多维度监控体系，具体监控哪些指标？告警阈值如何设置？如何避免告警风暴？遇到过最难定位的线上问题是什么？怎么解决的？

[监控与故障定位](监控与故障定位.md)

---

## 第五部分：业务理解与架构权衡（5个问题）

### 1. 商品模型的领域建模
外卖商品和电商商品有什么本质区别？你们的商品模型是如何设计的？SPU、SKU的概念在外卖场景下如何应用？商品的规格、属性、标签如何建模？

[商品模型的领域建模](商品模型的领域建模.md)

### 2. 商品与营销的解耦
商品系统需要支持满减、折扣、优惠券等营销活动，如何设计才能让商品系统不被营销逻辑污染？商品价格的计算逻辑在哪里？

[商品与营销的解耦](商品与营销的解耦.md)

### 3. 多租户架构设计
美团外卖有不同类型的商家（餐饮、超市、药店等），商品系统如何支持多业态？是用同一套系统还是分开？如何做到既统一又灵活？

[多租户架构设计](多租户架构设计.md)

### 4. 成本优化
4000+台服务器的成本不低，做过哪些成本优化？比如缓存命中率提升、机器资源利用率优化、存储成本优化等？效果如何？

[成本优化](成本优化.md)

### 5. 技术债务与重构
这么大规模的系统，肯定积累了不少技术债务。你认为当前商品系统最大的技术债务是什么？如果让你重新设计，会做哪些不同的选择？

[技术债务与重构](技术债务与重构.md)