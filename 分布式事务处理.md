# 分布式事务处理

## 核心要点（面试快速回答）

**Q: 商品系统和库存、订单系统之间的一致性如何保证？**
我们主要用最终一致性方案，核心是"本地消息表+MQ"。强一致性（2PC/TCC）性能开销太大，不适合高并发场景。具体做法：先写本地事务（业务数据+消息记录），再异步发MQ通知下游，下游处理完回调确认。通过补偿机制保证最终一致。

**Q: 为什么不用2PC或TCC？**
2PC有协调者单点问题，且同步阻塞影响性能；TCC需要业务改造成Try-Confirm-Cancel三阶段，侵入性强，对商品这种复杂业务改造成本高。最终一致性方案更轻量，能满足业务需求（秒级一致即可）。

**Q: 能举个具体业务场景吗？**
商品上架场景：商品状态更新（DB）→ 缓存刷新 → ES索引更新 → 通知推荐系统。我们用事件驱动：先写DB+本地消息表，再发MQ广播，各系统独立消费。任何一步失败都有重试和补偿。

**Q: 如何处理消息丢失或重复？**
消息丢失：定时扫描本地消息表，未确认的重新发送。消息重复：下游做幂等处理，通过唯一业务ID去重。

---

## 一、分布式事务的业务场景

### 1.1 商品系统涉及的跨系统交互

```
商品系统的上下游依赖：

上游系统（数据来源）：
├─ 商家系统：商品创建、编辑
├─ 运营系统：批量操作、活动配置
└─ 供应链系统：库存同步

下游系统（数据消费）：
├─ 库存系统：库存预占、扣减
├─ 订单系统：商品快照、价格校验
├─ 搜索系统：ES索引更新
├─ 推荐系统：商品特征更新
└─ 缓存系统：多级缓存刷新
```

### 1.2 典型事务场景

| 场景 | 涉及系统 | 一致性要求 | 采用方案 |
|-----|---------|-----------|---------|
| 商品上架 | 商品+缓存+ES+推荐 | 秒级最终一致 | 本地消息表+MQ |
| 商品下架 | 商品+缓存+ES+库存 | 秒级最终一致 | 本地消息表+MQ |
| 库存扣减 | 商品+库存+订单 | 强一致（库存准确） | TCC（仅库存链路） |
| 价格变更 | 商品+缓存+ES | 秒级最终一致 | 本地消息表+MQ |
| 批量导入 | 商品+缓存+ES | 分钟级最终一致 | 异步任务+补偿 |

---

## 二、为什么选择最终一致性

### 2.1 各方案对比

**2PC（两阶段提交）：**
```
优点：强一致性
缺点：
- 同步阻塞，性能差
- 协调者单点问题
- 网络分区时可能数据不一致

不适合原因：百万QPS下同步阻塞不可接受
```

**TCC（Try-Confirm-Cancel）：**
```
优点：无协调者单点，性能较好
缺点：
- 业务侵入性强，需要拆分三阶段
- 实现复杂，每个参与方都要实现Try/Confirm/Cancel
- 空回滚、悬挂等异常场景处理复杂

不适合原因：商品业务复杂，改造成本高
```

**本地消息表+MQ（最终一致性）：**
```
优点：
- 实现简单，业务侵入小
- 性能好，异步处理
- 可靠性高，消息持久化

缺点：
- 只能保证最终一致，有延迟窗口

适合原因：商品数据秒级延迟可接受
```

### 2.2 业务分析：为什么秒级延迟可接受？

```
场景分析：

商品上架后1秒内用户搜不到：
- 概率：极低（用户不会刚上架就搜）
- 影响：几乎无（1秒后就能搜到）
- 结论：可接受

商品下架后1秒内用户还能看到：
- 概率：低
- 影响：用户点击后提示"商品已下架"
- 结论：可接受（比系统不可用好）

价格变更后1秒内显示旧价格：
- 概率：低
- 影响：下单时会校验最新价格
- 结论：可接受（有兜底校验）
```

---

## 三、本地消息表方案详解

### 3.1 整体架构

```
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│  商品服务   │───▶│  消息队列   │───▶│  下游服务   │
│             │    │   (MQ)      │    │ (ES/缓存等) │
└─────────────┘    └─────────────┘    └─────────────┘
       │                                     │
       │ 同一事务                             │ 消费确认
       ▼                                     ▼
┌─────────────┐                       ┌─────────────┐
│  商品表     │                       │  处理结果   │
│  消息表     │◀──────────────────────│  回调确认   │
└─────────────┘                       └─────────────┘
```

### 3.2 本地消息表设计

**设计思路**：本地消息表是整个方案的关键，它和业务数据在同一个数据库，可以利用本地事务保证"业务操作"和"消息记录"要么同时成功，要么同时失败。这样即使MQ故障，消息也不会丢失，后续可以通过定时任务重新发送。

```sql
CREATE TABLE product_event_message (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    message_id VARCHAR(64) NOT NULL UNIQUE,  -- 消息唯一ID
    product_id BIGINT NOT NULL,              -- 商品ID
    event_type VARCHAR(32) NOT NULL,         -- 事件类型：CREATE/UPDATE/DELETE
    event_data TEXT,                         -- 事件数据（JSON）
    status TINYINT DEFAULT 0,                -- 0:待发送 1:已发送 2:已确认 3:失败
    retry_count INT DEFAULT 0,               -- 重试次数
    create_time DATETIME,
    update_time DATETIME,
    INDEX idx_status_time (status, create_time)
);
```

**关键字段说明**：
- **status字段**：用于追踪消息的生命周期，从待发送→已发送→已确认，如果中间失败可以重试
- **retry_count**：限制重试次数，避免死循环，超过次数后人工介入
- **idx_status_time索引**：定时任务会按状态和时间查询，这个索引可以加速查询

### 3.3 核心流程

**步骤1：业务操作+消息写入（同一事务）**

**这一步是整个方案的关键**：业务数据和消息记录在同一个数据库事务中写入。如果业务操作失败，消息也不会写入；如果消息写入失败，业务操作也会回滚。这保证了"要么都成功，要么都失败"。

**为什么不先发MQ再写DB**：因为MQ发送成功后DB可能失败，导致消息已发但数据没落地。用本地消息表可以保证消息一定持久化。

```java
@Transactional
public void updateProduct(ProductUpdateRequest request) {
    // 1. 更新商品数据
    Product product = productDao.selectById(request.getProductId());
    product.setName(request.getName());
    product.setPrice(request.getPrice());
    product.setUpdateTime(new Date());
    productDao.update(product);

    // 2. 写入本地消息表（同一事务）
    ProductEventMessage message = new ProductEventMessage();
    message.setMessageId(UUID.randomUUID().toString());
    message.setProductId(product.getId());
    message.setEventType("UPDATE");
    message.setEventData(JSON.toJSONString(product));
    message.setStatus(0);  // 待发送
    message.setCreateTime(new Date());
    messageDao.insert(message);

    // 事务提交后，消息一定存在
}
```

**步骤2：异步发送消息**

**为什么用定时任务而不是同步发送**：同步发送会增加接口RT，而且MQ可能暂时不可用。定时任务轮询待发送消息，失败了可以重试，不影响业务接口响应时间。

**发送成功后更新状态**：从"待发送"变为"已发送"，后续等待下游确认。

```java
@Scheduled(fixedRate = 1000)  // 每秒执行
public void sendPendingMessages() {
    // 查询待发送的消息
    List<ProductEventMessage> messages = messageDao.selectByStatus(0, 100);

    for (ProductEventMessage message : messages) {
        try {
            // 发送到MQ
            mqProducer.send("product_event_topic", message.getMessageId(),
                JSON.toJSONString(message));

            // 更新状态为已发送
            messageDao.updateStatus(message.getId(), 1);
        } catch (Exception e) {
            // 发送失败，增加重试次数
            messageDao.incrementRetryCount(message.getId());
            log.error("Send message failed: {}", message.getMessageId(), e);
        }
    }
}
```

**步骤3：下游消费+确认回调**

**幂等处理是关键**：MQ可能重复投递消息（网络问题、重试等），下游必须能处理重复消息。通过messageId去重，已处理过的消息直接跳过。

**回调确认的作用**：告诉上游"我处理成功了"，上游可以把消息状态更新为"已确认"。如果长时间没收到确认，上游会重新发送。

```java
// ES索引服务消费消息
@RocketMQMessageListener(topic = "product_event_topic")
public void onMessage(String messageBody) {
    ProductEventMessage message = JSON.parseObject(messageBody, ProductEventMessage.class);

    try {
        // 幂等检查
        if (isProcessed(message.getMessageId())) {
            return;
        }

        // 更新ES索引
        if ("UPDATE".equals(message.getEventType())) {
            Product product = JSON.parseObject(message.getEventData(), Product.class);
            esClient.index("product", product.getId().toString(), product);
        }

        // 记录已处理
        markAsProcessed(message.getMessageId());

        // 回调确认
        confirmCallback(message.getMessageId());

    } catch (Exception e) {
        log.error("Process message failed: {}", message.getMessageId(), e);
        throw e;  // 抛异常，MQ会重试
    }
}
```

### 3.4 异常处理

**消息发送失败：**
```java
// 定时扫描发送失败的消息，重新发送
@Scheduled(fixedRate = 60000)  // 每分钟
public void retryFailedMessages() {
    // 查询发送超过1分钟未确认的消息
    List<ProductEventMessage> messages = messageDao.selectUnconfirmed(60);

    for (ProductEventMessage message : messages) {
        if (message.getRetryCount() >= 3) {
            // 超过重试次数，标记失败，人工处理
            messageDao.updateStatus(message.getId(), 3);
            alertService.send("消息发送失败超过重试次数: " + message.getMessageId());
            continue;
        }

        // 重新发送
        try {
            mqProducer.send("product_event_topic", message.getMessageId(),
                JSON.toJSONString(message));
            messageDao.incrementRetryCount(message.getId());
        } catch (Exception e) {
            log.error("Retry send failed: {}", message.getMessageId(), e);
        }
    }
}
```

**消息重复消费（幂等处理）：**
```java
// 使用Redis记录已处理的消息
public boolean isProcessed(String messageId) {
    return redisTemplate.hasKey("processed:" + messageId);
}

public void markAsProcessed(String messageId) {
    // 设置24小时过期，避免无限增长
    redisTemplate.opsForValue().set("processed:" + messageId, "1", 24, TimeUnit.HOURS);
}
```

---

## 四、TCC在库存场景的应用

### 4.1 为什么库存要用TCC？

**库存的特殊性**：库存关系到是否超卖，超卖意味着卖出了不存在的商品，这是不可接受的。最终一致性方案有个问题：如果先扣库存再创建订单，订单失败后库存难以回滚（消息已发出）；如果先创建订单再扣库存，可能超卖。TCC通过"预留-确认-取消"三阶段，保证库存操作的原子性。

```
库存扣减的特殊性：
- 必须准确，不能超卖
- 涉及金钱，错误影响大
- 需要强一致性保证

最终一致性的问题：
- 如果先扣库存再创建订单，订单失败后库存已扣
- 如果先创建订单再扣库存，可能超卖

TCC的优势：
- Try阶段预留资源（冻结库存）
- Confirm阶段真正扣减
- Cancel阶段释放预留
- 保证库存准确
```

### 4.2 TCC实现

**TCC三阶段的含义**：
- **Try**：预留资源，把库存从"可用"移到"冻结"，但不真正扣减
- **Confirm**：确认扣减，把"冻结"的库存真正扣掉
- **Cancel**：释放预留，把"冻结"的库存还回"可用"

**为什么要"冻结"而不是直接扣**：Try阶段可能失败（库存不足），此时其他操作还能正常进行。如果直接扣减，失败回滚的过程中库存状态会不一致。

**Try阶段：冻结库存**
```java
public boolean tryDeductStock(Long productId, Integer quantity) {
    // 检查可用库存
    Stock stock = stockDao.selectById(productId);
    if (stock.getAvailable() < quantity) {
        return false;  // 库存不足
    }

    // 冻结库存：可用库存减少，冻结库存增加
    int affected = stockDao.freeze(productId, quantity);
    return affected > 0;
}

// SQL
UPDATE stock
SET available = available - #{quantity},
    frozen = frozen + #{quantity}
WHERE product_id = #{productId}
AND available >= #{quantity}
```

**Confirm阶段：确认扣减**
```java
public boolean confirmDeductStock(Long productId, Integer quantity) {
    // 冻结库存转为已扣减
    int affected = stockDao.confirm(productId, quantity);
    return affected > 0;
}

// SQL
UPDATE stock
SET frozen = frozen - #{quantity}
WHERE product_id = #{productId}
AND frozen >= #{quantity}
```

**Cancel阶段：释放库存**
```java
public boolean cancelDeductStock(Long productId, Integer quantity) {
    // 冻结库存释放回可用库存
    int affected = stockDao.cancel(productId, quantity);
    return affected > 0;
}

// SQL
UPDATE stock
SET available = available + #{quantity},
    frozen = frozen - #{quantity}
WHERE product_id = #{productId}
AND frozen >= #{quantity}
```

### 4.3 TCC异常处理

**空回滚问题：**
```
场景：Try未执行（超时），但Cancel被调用
解决：Cancel前检查Try是否执行过

if (!tryExecuted(productId, orderId)) {
    // Try未执行，直接返回成功
    return true;
}
```

**悬挂问题：**
```
场景：Cancel先执行（网络延迟），Try后到达
解决：Try前检查是否已Cancel

if (alreadyCancelled(productId, orderId)) {
    // 已Cancel，拒绝Try
    return false;
}
```

---

## 五、事件驱动架构

### 5.1 商品事件定义

```java
public enum ProductEventType {
    CREATED("商品创建"),
    UPDATED("商品更新"),
    DELETED("商品删除"),
    ON_SHELF("商品上架"),
    OFF_SHELF("商品下架"),
    PRICE_CHANGED("价格变更"),
    STOCK_CHANGED("库存变更");
}

@Data
public class ProductEvent {
    private String eventId;           // 事件ID
    private ProductEventType type;    // 事件类型
    private Long productId;           // 商品ID
    private Long merchantId;          // 商家ID
    private Map<String, Object> data; // 事件数据
    private Long timestamp;           // 事件时间
}
```

### 5.2 事件发布与订阅

```java
// 事件发布
@Component
public class ProductEventPublisher {

    @Autowired
    private RocketMQTemplate mqTemplate;

    public void publish(ProductEvent event) {
        // 发送到不同的Topic，按事件类型路由
        String topic = "product_event_" + event.getType().name().toLowerCase();
        mqTemplate.send(topic, JSON.toJSONString(event));
    }
}

// 事件订阅 - ES索引服务
@RocketMQMessageListener(topic = "product_event_on_shelf")
public class EsIndexEventListener {

    public void onMessage(String message) {
        ProductEvent event = JSON.parseObject(message, ProductEvent.class);
        // 商品上架，创建ES索引
        esService.indexProduct(event.getProductId());
    }
}

// 事件订阅 - 缓存服务
@RocketMQMessageListener(topic = "product_event_updated")
public class CacheEventListener {

    public void onMessage(String message) {
        ProductEvent event = JSON.parseObject(message, ProductEvent.class);
        // 商品更新，刷新缓存
        cacheService.refresh(event.getProductId());
    }
}
```

### 5.3 事件顺序性保证

```java
// 使用商家ID作为分区键，保证同一商家的事件顺序
public void publishOrdered(ProductEvent event) {
    String topic = "product_event_ordered";
    // 使用merchantId作为分区键
    mqTemplate.send(topic, event.getMerchantId().toString(),
        JSON.toJSONString(event));
}
```

---

## 六、补偿机制

### 6.1 定时对账

```java
@Scheduled(cron = "0 0 * * * ?")  // 每小时执行
public void reconcile() {
    // 获取最近1小时的商品变更
    List<Long> changedProductIds = getRecentChangedProducts(1, TimeUnit.HOURS);

    for (Long productId : changedProductIds) {
        // 比对DB和ES的数据
        Product dbProduct = productDao.selectById(productId);
        Product esProduct = esService.get(productId);

        if (!isConsistent(dbProduct, esProduct)) {
            log.warn("Data inconsistent for product: {}", productId);
            // 以DB为准，修复ES
            esService.indexProduct(productId);
            metrics.record("reconcile.fix.es");
        }

        // 比对DB和缓存的数据
        Product cacheProduct = cacheService.get(productId);
        if (!isConsistent(dbProduct, cacheProduct)) {
            log.warn("Cache inconsistent for product: {}", productId);
            // 刷新缓存
            cacheService.refresh(productId);
            metrics.record("reconcile.fix.cache");
        }
    }
}
```

### 6.2 人工补偿入口

```java
@RestController
@RequestMapping("/admin/compensate")
public class CompensateController {

    // 单个商品补偿
    @PostMapping("/product/{productId}")
    public Result compensateProduct(@PathVariable Long productId) {
        // 刷新缓存
        cacheService.refresh(productId);
        // 更新ES
        esService.indexProduct(productId);
        // 通知下游
        eventPublisher.publish(new ProductEvent(ProductEventType.UPDATED, productId));

        return Result.success();
    }

    // 批量补偿
    @PostMapping("/batch")
    public Result compensateBatch(@RequestBody List<Long> productIds) {
        for (Long productId : productIds) {
            compensateProduct(productId);
        }
        return Result.success();
    }
}
```

---

## 七、实战案例

### 7.1 案例：商品上架的完整流程

```
商家在B端点击"上架"按钮

1. 商品服务处理
   ├─ 校验商品信息完整性
   ├─ 更新商品状态为"上架"
   ├─ 写入本地消息表
   └─ 返回商家"上架成功"

2. 异步消息发送（1秒内）
   └─ 发送到MQ：product_event_on_shelf

3. 各系统消费处理
   ├─ 缓存服务：刷新商品缓存（100ms）
   ├─ ES服务：创建/更新索引（200ms）
   ├─ 推荐服务：更新商品特征（500ms）
   └─ 各服务回调确认

4. 用户可见（总耗时约1-2秒）
   └─ 用户搜索可以找到该商品
```

### 7.2 案例：消息丢失的处理

```
场景：MQ故障导致消息丢失

发现：
- 定时任务扫描发现消息状态为"已发送"超过5分钟未确认
- 触发告警

处理：
1. 自动重试发送（最多3次）
2. 重试失败后标记为"失败"
3. 人工介入处理
4. 通过补偿接口修复数据

预防：
- 本地消息表保证消息不丢
- 定时对账发现不一致
- 监控消息积压情况
```

---

## 八、最佳实践

### 8.1 方案选择原则

```
选择最终一致性：
- 业务可接受秒级延迟
- 系统间解耦要求高
- QPS高，性能要求高

选择TCC：
- 涉及资金、库存等关键数据
- 必须强一致性
- 可以接受业务改造成本

选择2PC：
- 传统系统，改造成本高
- QPS不高，可以接受性能损耗
- 已有分布式事务中间件支持
```

### 8.2 实施要点

1. **消息必须持久化**：本地消息表保证消息不丢
2. **下游必须幂等**：通过唯一ID去重
3. **必须有补偿机制**：定时对账+人工入口
4. **必须有监控告警**：消息积压、处理延迟、不一致数量
